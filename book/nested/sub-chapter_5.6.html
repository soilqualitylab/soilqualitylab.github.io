<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Section 6: Cloud Deployment Strategies - Listen To Yourself</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Listen To Yourself</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h2 id="cloud-deployment-strategies"><a class="header" href="#cloud-deployment-strategies">Cloud Deployment Strategies</a></h2>
<p>You also may want to look at <em><strong>other</strong></em> Sections:</p>
<ul>
<li><a href="sub-chapter_5.1.html">Section 1: Foundations of Local Development for ML/AI</a></li>
<li><a href="sub-chapter_5.2.html">Section 2: Hardware Optimization Strategies</a></li>
<li><a href="sub-chapter_5.3.html">Section 3: Local Development Environment Setup</a></li>
<li><a href="sub-chapter_5.4.html">Section 4: Model Optimization Techniques</a></li>
<li><a href="sub-chapter_5.5.html">Section 5: MLOps Integration and Workflows</a></li>
<li><a href="sub-chapter_5.7.html">Section 7: Real-World Case Studies</a></li>
<li><a href="sub-chapter_5.8.html">Section 8: Future Trends and Advanced Topics</a></li>
</ul>
<h3 id="post-81-cloud-provider-selection-for-mlai-workloads"><a class="header" href="#post-81-cloud-provider-selection-for-mlai-workloads">Post 81: Cloud Provider Selection for ML/AI Workloads</a></h3>
<p>This post provides a comprehensive framework for selecting the optimal cloud provider for ML/AI deployment after local development, emphasizing that ML workloads have specialized requirements distinct from general cloud computing. It examines the critical comparison factors across major providers (AWS, GCP, Azure) and specialized ML platforms (SageMaker, Vertex AI, RunPod, VAST.ai) including GPU availability/variety, pricing structures, ML-specific tooling, and integration capabilities with existing workflows. The post analyzes the strengths and weaknesses of each provider for different ML workload types, showing where specialized providers like RunPod offer significant cost advantages for specific scenarios (training) while major providers excel in production-ready infrastructure and compliance. It provides a structured decision framework that helps teams select providers based on workload type, scale requirements, budget constraints, and existing technology investments rather than defaulting to familiar providers that may not offer optimal price-performance for ML/AI workloads.</p>
<h3 id="post-82-specialized-gpu-cloud-providers-for-cost-savings"><a class="header" href="#post-82-specialized-gpu-cloud-providers-for-cost-savings">Post 82: <a href="nested/sub-chapter_5.6.82.html">Specialized GPU Cloud Providers for Cost Savings</a></a></h3>
<p>This post examines the unique operational models of specialized GPU cloud providers like RunPod, VAST.ai, ThunderCompute, and Lambda Labs that offer dramatically different cost structures and hardware access compared to major cloud providers. It explores how these specialized platforms leverage marketplace approaches, spot pricing models, and direct hardware access to deliver GPU resources at prices typically 3-5x lower than major cloud providers for equivalent hardware. The post details practical usage patterns for these platforms, including job specification techniques, data management strategies, resilience patterns for handling potential preemption, and effective integration with broader MLOps workflows. It provides detailed cost-benefit analysis across providers for common ML workloads, demonstrating scenarios where these specialized platforms can reduce compute costs by 70-80% compared to major cloud providers, particularly for research, experimentation, and non-production workloads where their infrastructure trade-offs are acceptable.</p>
<h3 id="post-83-managing-cloud-costs-for-mlai-workloads"><a class="header" href="#post-83-managing-cloud-costs-for-mlai-workloads">Post 83: Managing Cloud Costs for ML/AI Workloads</a></h3>
<p>This post presents a systematic approach to managing and optimizing cloud costs for ML/AI workloads, which can escalate rapidly without proper governance due to their resource-intensive nature. It explores comprehensive cost optimization strategies including infrastructure selection, workload scheduling, resource utilization patterns, and deployment architectures that dramatically reduce cloud expenditure without compromising performance. The post details implementation techniques for specific cost optimization methods including spot/preemptible instance usage, instance right-sizing, automated shutdown policies, storage lifecycle management, caching strategies, and efficient data transfer patterns with quantified impact on overall spending. It provides frameworks for establishing cost visibility, implementing budget controls, and creating organizational accountability mechanisms that maintain financial control throughout the ML lifecycle, preventing the common scenario where cloud costs unexpectedly spiral after initial development, forcing projects to be scaled back or abandoned despite technical success.</p>
<h3 id="post-84-hybrid-training-strategies"><a class="header" href="#post-84-hybrid-training-strategies">Post 84: Hybrid Training Strategies</a></h3>
<p>This post examines hybrid training architectures that strategically distribute workloads between local hardware and cloud resources to optimize for both cost efficiency and computational capability. It explores various hybrid training patterns including local prototyping with cloud scaling, distributed training across environments, parameter server architectures, and federated learning approaches that leverage the strengths of both environments. The post details technical implementation approaches for these hybrid patterns, including data synchronization mechanisms, checkpoint management, distributed training configurations, and workflow orchestration tools that maintain consistency across heterogeneous computing environments. It provides decision frameworks for determining optimal workload distribution based on model architectures, dataset characteristics, training dynamics, and available resource profiles, enabling teams to achieve maximum performance within budget constraints by leveraging each environment for the tasks where it provides the greatest value rather than defaulting to a simplistic all-local or all-cloud approach.</p>
<h3 id="post-85-cloud-based-fine-tuning-pipelines"><a class="header" href="#post-85-cloud-based-fine-tuning-pipelines">Post 85: Cloud-Based Fine-Tuning Pipelines</a></h3>
<p>This post provides a comprehensive blueprint for implementing efficient cloud-based fine-tuning pipelines that adapt foundation models to specific domains after initial local development and experimentation. It explores architectural patterns for optimized fine-tuning workflows including data preparation, parameter-efficient techniques (LoRA, QLoRA, P-Tuning), distributed training configurations, evaluation frameworks, and model versioning specifically designed for cloud execution. The post details implementation approaches for these pipelines across different cloud environments, comparing managed services (SageMaker, Vertex AI) against custom infrastructure with analysis of their respective trade-offs for different organization types. It provides guidance on implementing appropriate monitoring, checkpointing, observability, and fault tolerance mechanisms that ensure reliable execution of these resource-intensive jobs, enabling organizations to adapt models at scales that would be impractical on local hardware while maintaining integration with the broader ML workflow established during local development.</p>
<h3 id="post-86-cloud-inference-api-design-and-implementation"><a class="header" href="#post-86-cloud-inference-api-design-and-implementation">Post 86: Cloud Inference API Design and Implementation</a></h3>
<p>This post examines best practices for designing and implementing high-performance inference APIs that efficiently serve models in cloud environments after local development and testing. It explores API architectural patterns including synchronous vs. asynchronous interfaces, batching strategies, streaming responses, and caching approaches that optimize for different usage scenarios and latency requirements. The post details implementation approaches using different serving frameworks (TorchServe, Triton Inference Server, TensorFlow Serving) and deployment options (container services, serverless, dedicated instances) with comparative analysis of their performance characteristics, scaling behavior, and operational complexity. It provides guidance on implementing robust scaling mechanisms, graceful degradation strategies, reliability patterns, and observability frameworks that ensure consistent performance under variable load conditions without requiring excessive overprovisioning. These well-designed inference APIs form the critical bridge between model capabilities and application functionality, enabling the value created during model development to be effectively delivered to end-users with appropriate performance, reliability, and cost characteristics.</p>
<h3 id="post-87-serverless-deployment-for-mlai-workloads"><a class="header" href="#post-87-serverless-deployment-for-mlai-workloads">Post 87: Serverless Deployment for ML/AI Workloads</a></h3>
<p>This post explores serverless architectures for deploying ML/AI workloads to cloud environments with significantly reduced operational complexity compared to traditional infrastructure approaches. It examines the capabilities and limitations of serverless platforms (AWS Lambda, Azure Functions, Google Cloud Functions, Cloud Run) for different ML tasks, including inference, preprocessing, orchestration, and event-driven workflows. The post details implementation strategies for deploying models to serverless environments, including packaging approaches, memory optimization, cold start mitigation, execution time management, and efficient handler design specifically optimized for ML workloads. It provides architectural patterns for decomposing ML systems into serverless functions that effectively balance performance, cost, and operational simplicity while working within the constraints imposed by serverless platforms. This approach enables teams to deploy models with minimal operational overhead after local development, allowing smaller organizations to maintain production ML systems without specialized infrastructure expertise while automatically scaling to match demand patterns with pay-per-use pricing.</p>
<h3 id="post-88-container-orchestration-for-mlai-workloads"><a class="header" href="#post-88-container-orchestration-for-mlai-workloads">Post 88: Container Orchestration for ML/AI Workloads</a></h3>
<p>This post provides a detailed guide to implementing container orchestration solutions for ML/AI workloads that require more flexibility and customization than serverless approaches can provide. It examines orchestration platforms (Kubernetes, ECS, GKE, AKS) with comparative analysis of their capabilities for managing complex ML deployments, including resource scheduling, scaling behavior, and operational requirements. The post details implementation patterns for efficiently containerizing ML components, including resource allocation strategies, pod specifications, scaling policies, networking configurations, and deployment workflows optimized for ML-specific requirements like GPU access and distributed training. It provides guidance on implementing appropriate monitoring, logging, scaling policies, and operational practices that ensure reliable production operation with manageable maintenance overhead. This container orchestration approach provides a middle ground between the simplicity of serverless and the control of custom infrastructure, offering substantial flexibility and scaling capabilities while maintaining reasonable operational complexity for teams with modest infrastructure expertise.</p>
<h3 id="post-89-model-serving-at-scale"><a class="header" href="#post-89-model-serving-at-scale">Post 89: Model Serving at Scale</a></h3>
<p>This post examines architectural patterns and implementation strategies for serving ML models at large scale in cloud environments, focusing on achieving high-throughput, low-latency inference for production applications. It explores specialized model serving frameworks (NVIDIA Triton, KServe, TorchServe) with detailed analysis of their capabilities for addressing complex serving requirements including ensemble models, multi-model serving, dynamic batching, and hardware acceleration. The post details technical approaches for implementing horizontal scaling, load balancing, request routing, and high-availability configurations that efficiently distribute inference workloads across available resources while maintaining resilience. It provides guidance on performance optimization techniques including advanced batching strategies, caching architectures, compute kernel optimization, and hardware acceleration configuration that maximize throughput while maintaining acceptable latency under variable load conditions. This scalable serving infrastructure enables models developed locally to be deployed in production environments capable of handling substantial request volumes with predictable performance characteristics and efficient resource utilization regardless of demand fluctuations.</p>
<h3 id="post-90-cloud-security-for-mlai-deployments"><a class="header" href="#post-90-cloud-security-for-mlai-deployments">Post 90: Cloud Security for ML/AI Deployments</a></h3>
<p>This post provides a comprehensive examination of security considerations specific to ML/AI deployments in cloud environments, addressing both traditional cloud security concerns and emerging ML-specific vulnerabilities. It explores security challenges throughout the ML lifecycle including training data protection, model security, inference protection, and access control with detailed analysis of their risk profiles and technical mitigation strategies. The post details implementation approaches for securing ML workflows in cloud environments including encryption mechanisms (at-rest, in-transit, in-use), network isolation configurations, authentication frameworks, and authorization models appropriate for different sensitivity levels and compliance requirements. It provides guidance on implementing security monitoring, vulnerability assessment, and incident response procedures specifically adapted for ML systems to detect and respond to unique threat vectors like model extraction, model inversion, or adversarial attacks. These specialized security practices ensure that models deployed to cloud environments after local development maintain appropriate protection for both the intellectual property represented by the models and the data they process, addressing the unique security considerations of ML systems beyond traditional application security concerns.</p>
<h3 id="post-91-edge-deployment-from-cloud-trained-models"><a class="header" href="#post-91-edge-deployment-from-cloud-trained-models">Post 91: Edge Deployment from Cloud-Trained Models</a></h3>
<p>This post examines strategies for efficiently deploying cloud-trained models to edge devices, extending ML capabilities to environments with limited connectivity, strict latency requirements, or data privacy constraints. It explores the technical challenges of edge deployment including model optimization for severe resource constraints, deployment packaging for diverse hardware targets, and update mechanisms that bridge the capability gap between powerful cloud infrastructure and limited edge execution environments. The post details implementation approaches for different edge targets ranging from mobile devices to embedded systems to specialized edge hardware, with optimization techniques tailored to each platform's specific constraints. It provides guidance on implementing hybrid edge-cloud architectures that intelligently distribute computation between edge and cloud components based on network conditions, latency requirements, and processing complexity. This edge deployment capability extends the reach of models initially developed locally and refined in the cloud to operate effectively in environments where cloud connectivity is unavailable, unreliable, or introduces unacceptable latency, significantly expanding the potential application domains for ML systems.</p>
<h3 id="post-92-multi-region-deployment-strategies"><a class="header" href="#post-92-multi-region-deployment-strategies">Post 92: Multi-Region Deployment Strategies</a></h3>
<p>This post explores strategies for deploying ML systems across multiple geographic regions to support global user bases with appropriate performance and compliance characteristics. It examines multi-region architectures including active-active patterns, regional failover configurations, and traffic routing strategies that balance performance, reliability, and regulatory compliance across diverse geographic locations. The post details technical implementation approaches for maintaining model consistency across regions, managing region-specific adaptations, implementing appropriate data residency controls, and addressing divergent regulatory requirements that impact model deployment and operation. It provides guidance on selecting appropriate regions, implementing efficient deployment pipelines for coordinated multi-region updates, and establishing monitoring systems that provide unified visibility across the distributed infrastructure. This multi-region approach enables models initially developed locally to effectively serve global user bases with appropriate performance and reliability characteristics regardless of user location, while addressing the complex regulatory and data governance requirements that often accompany international operations without requiring multiple isolated deployment pipelines.</p>
<h3 id="post-93-hybrid-cloud-strategies-for-mlai"><a class="header" href="#post-93-hybrid-cloud-strategies-for-mlai">Post 93: Hybrid Cloud Strategies for ML/AI</a></h3>
<p>This post examines hybrid cloud architectures that strategically distribute ML workloads across multiple providers or combine on-premises and cloud resources to optimize for specific requirements around cost, performance, or data sovereignty. It explores architectural patterns for hybrid deployments including workload segmentation, data synchronization mechanisms, and orchestration approaches that maintain consistency and interoperability across heterogeneous infrastructure. The post details implementation strategies for effectively managing hybrid environments, including identity federation, network connectivity options, and monitoring solutions that provide unified visibility and control across diverse infrastructure components. It provides guidance on workload placement decision frameworks, migration strategies between environments, and operational practices specific to hybrid ML deployments that balance flexibility with manageability. This hybrid approach provides maximum deployment flexibility after local development, enabling organizations to leverage the specific strengths of different providers or infrastructure types while avoiding single-vendor lock-in and optimizing for unique requirements around compliance, performance, or cost that may not be well-served by a single cloud provider.</p>
<h3 id="post-94-automatic-model-retraining-in-the-cloud"><a class="header" href="#post-94-automatic-model-retraining-in-the-cloud">Post 94: Automatic Model Retraining in the Cloud</a></h3>
<p>This post provides a detailed blueprint for implementing automated retraining pipelines that continuously update models in cloud environments based on new data, performance degradation, or concept drift without requiring manual intervention. It explores architectural patterns for continuous retraining including performance monitoring systems, drift detection mechanisms, data validation pipelines, training orchestration, and automated deployment systems that maintain model relevance over time. The post details implementation approaches for these pipelines using both managed services and custom infrastructure, with strategies for ensuring training stability, preventing quality regression, and managing the transition between model versions. It provides guidance on implementing appropriate evaluation frameworks, approval gates, champion-challenger patterns, and rollback mechanisms that maintain production quality while enabling safe automatic updates. This continuous retraining capability ensures models initially developed locally remain effective as production data distributions naturally evolve, extending model useful lifespan and reducing maintenance burden without requiring constant developer attention to maintain performance in production environments.</p>
<h3 id="post-95-disaster-recovery-for-mlai-systems"><a class="header" href="#post-95-disaster-recovery-for-mlai-systems">Post 95: Disaster Recovery for ML/AI Systems</a></h3>
<p>This post examines comprehensive disaster recovery strategies for ML/AI systems deployed to cloud environments, addressing the unique recovery requirements distinct from traditional applications. It explores DR planning methodologies for ML systems, including recovery priority classification frameworks, RTO/RPO determination guidelines, and risk assessment approaches that address the specialized components and dependencies of ML systems. The post details technical implementation approaches for ensuring recoverability including model serialization practices, training data archiving strategies, pipeline reproducibility mechanisms, and state management techniques that enable reliable reconstruction in disaster scenarios. It provides guidance on testing DR plans, implementing specialized backup strategies for large artifacts, and documenting recovery procedures specific to each ML system component. These disaster recovery practices ensure mission-critical ML systems deployed to cloud environments maintain appropriate business continuity capabilities, protecting the substantial investment represented by model development and training while minimizing potential downtime or data loss in disaster scenarios in a cost-effective manner proportional to the business value of each system.</p>
<h3 id="post-96-cloud-provider-migration-strategies"><a class="header" href="#post-96-cloud-provider-migration-strategies">Post 96: Cloud Provider Migration Strategies</a></h3>
<p>This post provides a practical guide for migrating ML/AI workloads between cloud providers or from cloud to on-premises infrastructure in response to changing business requirements, pricing conditions, or technical needs. It explores migration planning frameworks including dependency mapping, component assessment methodologies, and phased transition strategies that minimize risk and service disruption during provider transitions. The post details technical implementation approaches for different migration patterns including lift-and-shift, refactoring, and hybrid transition models with specific consideration for ML-specific migration challenges around framework compatibility, hardware differences, and performance consistency. It provides guidance on establishing migration validation frameworks, conducting proof-of-concept migrations, and implementing rollback capabilities that ensure operational continuity throughout the transition process. This migration capability prevents vendor lock-in after cloud deployment, enabling organizations to adapt their infrastructure strategy as pricing, feature availability, or regulatory requirements evolve without sacrificing the ML capabilities developed through their local-to-cloud workflow or requiring substantial rearchitecture of production systems.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../nested/sub-chapter_5.5.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../nested/nested/sub-chapter_5.6.82.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../nested/sub-chapter_5.5.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../nested/nested/sub-chapter_5.6.82.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
